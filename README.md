# ETL-data-pipeline-with-Apache-Airflow

## `Scenario`
A data driven company desires to analyze its web server log file. An ETL data pipeline was designed and implemented with Apache Airflow to filter out specific strings in the web server log file, save the output in a new file and archive the output file for future use.

## `Steps Taken`
- Apache Airflow was started with Linux bash commands
- A Python file was created and the DAG arguments, definition and tasks was submitted
- The Python file was submitted to Apache Airflow
- Progress of tasks execution was monitored via the Apache Airflow User Interface
